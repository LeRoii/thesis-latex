% !TeX root = ../main.tex
\xchapter{基于时空Transformer网络的单模态轨迹预测}{Spatio-Temporal Transformer Network for Unimodal Trajectory Prediction}

为了安全合理地参与城市密集交通场景，自动驾驶汽车需要精准分析周围不同类别交通参与者的运动模式，然后准确预测出这些交通参与者在未来一段时间内的行为轨迹。在下文中，不同类别的交通参与者也称为异质交通参与者。异质交通参与者的轨迹不仅受到其自身物理约束因素影响，而且受到不同异质交通参与者之间的交互因素影响。因此，同时预测出自动驾驶汽车周围所有异质交通参与者的轨迹是非常具有挑战性的任务。基于深度学习的方法通常会利用长短期记忆网络（LSTM）对轨迹数据进行串行处理，并只针对单一类别的交通参与者进行建模，造成其预测性能差，适用范围窄。针对此问题，本章设计了基于时空Transformer网络的深度学习方法，通过构建时空图捕捉异质交通参与者的时空交互特征，从而为简单场景的决策规划提供精准的单模态轨迹预测输入。

\xsection{引言}{Introduction}
为了实现预期任务目标同时避免与其他交通参与者发生碰撞，自动驾驶汽车需要具备环境感知以及规划决策的能力。作为环境感知的最后环节，单模态轨迹预测可以提供自动驾驶汽车周围交通参与者的单一确定性行为预测轨迹，为规划模块做出安全合理的决策提供必要输入，从而架起感知到决策规划的桥梁，为保证自动驾驶系统的安全性发挥举足轻重的作用。然而，在进行复杂时间预测的同时，不可避免地存在交通参与者之间的空间交互影响。特别是在由异质交通参与者组成的密集且高度动态的城市交通环境中。异质交通参与者主要包括行人、非机动车辆和人类驾驶员。交通场景的异质性意味着这些交通参与者具有不同的类别、形状、运动学特征和行为特征等。因此，单模态轨迹预测是一项具有挑战性的任务。

传统的单模态轨迹预测方法将交通参与者视为没有任何空间交互的独立个体，并将运动抽象为运动学模型、动力学模型\cite{brannstrom2010model}、卡尔曼滤波\cite{kalerma_liu}、DS推理\cite{dssss}或高斯过程\cite{rasmussen2003gaussian}等。这些传统的单模态轨迹预测方法难以完成长期轨迹预测，从而不能处理城市复杂交通场景。随着深度神经网络的成功，出现了大量基于深度学习的单模态轨迹预测方法\cite{slstm,stgat,ivanovic2019trajectron,sstgcnn,lstm_ji}。这些方法聚焦于使用深度学习网络提取轨迹数据的时空维度特征。其中，长短期记忆网络被广泛用于时间维度的特征建模。对于具有时间顺序的轨迹数据，长短期记忆网络会进行串行处理，从而生成存储深度特征的隐状态向量。这种隐状态向量能够表示交通参与者的相关运动特征\cite{transformer}。然而，基于长短期记忆网络的方法使用的这种单向量存储方式记忆能力有限，通常难以处理复杂的时间依赖性\cite{attention}。在空间交互方面，池化机制\cite{deo2018convolutional}、注意力机制\cite{ivanovic2019trajectron,hanhao,lilin}和图卷积机制\cite{shanhui}都可以模拟交通参与者之间的空间作用力。这些方法的局限性在于其大部分仅对预先设定好的空间邻域内的交通参与者之间进行交互特征提取\cite{lianjing}，忽略了超出给定空间范围限制外的交通参与者影响。这个假设在交通参与者的速度很低时是成立的。随着速度的增加，这种预定义的空间邻域模式会失效。此外，大多数轨迹预测方法只针对交通场景中的同质交通参与者进行预测，包括密集行人\cite{slstm}或者高速公路上的车辆\cite{deo2018convolutional}。这些方法在处理大量异质交通参与者同时参与的城市密集交通环境方面具有很大的局限性。

为了解决上述时间和空间交互方面的问题，本章提出了一种时空Transformer网络(Spatio-Temporal Transformer Network，S2TNet)，用于预测针对异质交通参与者的单模态轨迹。S2TNet是基于原始的Transformer网络框架提出的，摒弃了LSTM网络对于轨迹数据的顺序处理结构。除历史轨迹之外，S2TNet网络还利用了额外的形状、航向、类别等输入特征来处理交通参与者的异质性。在空间维度上，S2TNet引入了空间注意力机制。该机制不仅能够提取自动驾驶车辆空间预定义邻域内的交互特征，而且可以提取到感知范围内所有交通参与者之间的空间交互特征。在时间维度上，S2TNet采用了时间卷积网络（Temporal Convolutional Networks，TCN）提取连续帧的时间依赖性。时间卷积网络和空间注意力相结合共同形成了时空Transformer编码器，两者的交替使用让时空特征不断融合更新。经过时空Transformer编码器处理后，基于时间注意力机制的时间Transformer编码器可以进一步独立提炼每个异质交通参与者的深度时间特征。时间Transformer解码器利用这些深度时间特征产生了单模态预测轨迹。

本章的主要贡献总结如下：
\begin{itemize}
\item 提出了一种用于异质交通参与者的单模态轨迹预测方法，采用基于Transformer的网络结构在整个自动驾驶汽车感知范围内构建的全域时空图模型上准确提取了时空交互信息。
\item 设计了时空特征提取能力强的时空Transformer编码器。在空间交互上，利用空间注意力子层的消息传递机制得到同一时刻场景内所有交通参与者之间传递的有效消息。在时间交互方面，引入时间卷积子层进行时间特征提取。两种子层的交替使用使得时间特征和空间特征进行了充分的融合，从而产生全新的时空特征。
\item 利用了可分离卷积的深度映射能力，通过时间Transformer编码解码结构进一步捕捉到深度特征的时间依赖性，并输出任意指定长度的未来轨迹。
\item 相比于之前性能最好的方法，S2TNet网络在公开的ApolloScape轨迹数据集上的加权平均位移误差(WSFDE)和加权最终位移误差(WSFDE)分别降低了7.2\%和7.7\%。
% \item S2TNet网络在CARLA仿真器和西安交通大学“发现号”自动驾驶车辆平台上成功部署。
\end{itemize}

\xsection{相关工作}{Related Work}
\paragraph{空间交互建模} 
作为开创性工作，具有吸引力和排斥力的社会力模型\cite{LOHNER2010366}已经在各种应用中被证明是有效的，包括行人运动分析\cite{slstm}和机器人领域\cite{6696576}等。该模型假设交通参与者可以被虚拟力驱动以进行目标导航和避撞。社会力模型在交互建模方面表现良好，但是在轨迹预测方面表现不佳。ORCA\cite{ORCA}和PORCA\cite{8403274}等基于几何的交互建模方法考虑交通参与者的几何形状并将交互建模转换为优化问题。部分使用连续动力学\cite{treuille2006continuum}或高斯过程\cite{4359316}的方法也相继被提出。Bera等人\cite{7487768}结合集成卡尔曼滤波器和行人运动模型来预测行人轨迹，用于分析不同场景下的行人运动，例如购物中心、广场和步行街等。还有部分方法可以识别不同类别的驾驶员行为\cite{cheung2018identifying}。为了将这些方法扩展到一般交通场景，Ma等人\cite{maaaa}通过考虑运动学和动态约束来预测多个交通参与者的轨迹。然而，经典方法的主要局限之一是依赖于人为设计的特征，这些特征很难调优和泛化。

\paragraph{基于循环神经网络的单模态轨迹预测}
循环神经网络（RNN）及其变体结构LSTM和GRU在单模态轨迹预测方面取得了很大进展。作为最早用于单模态轨迹预测的RNN之一，Social-LSTM\cite{slstm}通过基于空间网格的池化方案聚合被预测者周围所有交通参与者的隐特征向量输出，从而成功提取到了空间邻域内的社会交互特征。然而这种人工设计的池化机制效率低下，并且无法捕获全局上下文信息。TrafficPredict\cite{ma2019trafficpredict}利用LSTM提取得到同类交通参与者运动模式的共性特征，并将其细化为类别特征用于异质交通参与者预测。SR-LSTM\cite{zhang2019sr}引入了一种消息传递和选择机制来获取当前邻域内的关键意图。将RNN与卷积网络(CNN)或图神经网络(GNN)相结合可以扩展成混合网络用于单模态轨迹预测。Traphic\cite{chandra2019traphic}使用CNN提取池化层的深度，并通过LSTM进行基于行为的轨迹预测。Social-BiGAT\cite{kosaraju2019social}基于图注意力网络捕捉行人之间的社会交互信息。GRIP\cite{grip}直接将交通参与者的历史轨迹建模为时空图，并基于时空图卷积网络预测未来轨迹。

\paragraph{基于Transformer网络的单模态轨迹预测} 
Transformer网络\cite{attention}以其特有的注意力机制在自然语言处理领域内取得了卓越性能。除此之外，许多视觉Transformer网络变体还可以用来处理视觉任务，例如图像分类、对象检测和实例分割。近年来，Transformer网络也被应用于单模态轨迹预测任务。在不考虑任何复杂交互信息的情况下，原始的Transformer\cite{transformer}可以在行人轨迹预测上取得了良好的结果。STAR\cite{star}设计了一种空间图卷积编码器，结合原始的时间Transformer编码器，两种编码器分别可以用于时空交互建模。Su等人\cite{su2016crowd}计算了相邻交通参与者的速度相关性，并通过自注意力机制提取出了速度相似度高的邻居。Interaction Transformer\cite{li2020end}将Transformer网络和RNN相结合，有效地提取了交通参与者之间的时空依赖性，同时减少了交通参与者之间的碰撞。

\xsection{时空Transformer网络}{Proposed S2TNet Model}
根据交通参与者的历史轨迹和其他有用信息，如形状和类别等，S2TNet网络的目标是准确预测出交通参与者的单模态轨迹。整体任务的公式化描述与第2章\ref{PredictionTask}小节相似。不同之处在于，除了式(\ref{inputsss})中包含的历史轨迹输入，S2TNet网络每个交通参与者的输入还包含了长度$l$，宽度$w$，航向角$\theta$和类别$\tau$五种可用信息。S2TNet网络目前考虑了五种类别的交通参与者$\tau\in{(1,2,3,4,5)}$，依次代表小型车辆、大型车辆、行人、非机动车和其他类别。历史特征输入表示为：
\begin{equation}
    \textbf{\textit{o}}^n_t = (x^n_t,y^n_t,l^n_t, w^n_t, \theta^n_t, \tau^n_t)
\end{equation}
式中：$\textit{n}\in\{1，\cdots，N\}$，交通参与者总数\textit{N}随场景变化而变化。S2TNet网络通过消融实验证明：可用的附加特征有助于表达交通参与者的异质性并提高预测轨迹准确性。

S2TNet网络的整体框架如图\ref{fig:3_arc}所示。整个网络可以看成是编码器-解码器架构，包括时空Transformer编码器、时间Transformer编码器和时间Transformer解码器三个部分。首先，通过全连接层网络将历史特征集合\textit{\textbf{X}}的所有特征向量映射到深度特征空间中以获得丰富的运动信息。然后，在时空Transformer编码器中，空间注意力子层提取出相同时间内不同交通参与者的空间交互特征，时间卷积子层提取出同一交通参与者的时间交互特征。时空Transformer编码器通过交错使用空间注意力子层和时间卷积子层有效融合了不同的时空特征。经过时间Transformer编码器进一步提取时间维度上的深度特征后，时间Transformer解码器根据编码器所提供的时空深度特征，采用自回归的方式输出所有交通参与者的未来轨迹$\hat{\textit{\textbf{Y}}}$。

\begin{figure}[H]
\centering
\includegraphics[height=9.8cm]{3_arc.pdf}
\caption{S2TNet网络示意图}
\label{fig:3_arc}
\end{figure}

S2TNet网络在包含\textit{N}个交通参与者$T_o$帧历史特征的轨迹序列上构建了整个感知范围内的全域时空图模型。
\begin{equation}
    \textit{\textbf{G}}=(\textit{\textbf{V}},\textit{\textbf{E}})
\end{equation}

在时空图中，每个交通参与者的单帧轨迹输入表示一个节点。在本章之后的描述中，节点即指代交通参与者。\textbf{\textit{V}}表示所有\textit{N}个交通参与者的$T_o$帧历史数据组成的节点集合。
\begin{equation}
    \textit{\textbf{V}} = \{\textit{\textbf{o}}^n_t\,| \,t\in(1,T_o),i\in(1,N)\}
\end{equation}

\textbf{\textit{E}}表示连接节点与节点之间的边的集合。边集\textbf{\textit{E}}由两个子集组成。第一个子集是同一时刻内所有交通参与者节点之间的空间连接。
\begin{equation}
    \textit{\textbf{E}}_S=\{\textit{\textbf{o}}^i_t,\textit{\textbf{o}}^j_t)\,| \,i,j\in(1,N),t\in(1,T_o)\}
\end{equation}

第二个子集是不同时刻相同交通参与者的时间连接。
\begin{equation}
    \textit{\textbf{E}}_T=\{\textit{\textbf{o}}^i_t,\textit{\textbf{o}}_i^{t_1}\,| \,i\in(1,N),t,t_1\in(1,T_o)\}
\end{equation}

\xsubsection{时空Transformer编码器}{Spatio-temporal Transformer Encoder} \label{sptial_att}
时空Transformer编码器可以有效地处理与时间连续性特征相耦合的空间交互。时空Transformer编码器由$N_h=8$组相同的网络层组成。每组网络层包含两个子层。第一个子层是空间注意力子层，用于提取不同交通参与者相同时刻的空间特征信息。第二子层是时间卷积子层，用于提取同一交通参与者时间维度上的深度特征。每个子层之间顺序经过残差连接和层标准化。通过两个子层的交错使用，时空信息特征可以充分融合，从而进一步提高网络性能。

在输入时空Transformer编码器之前，原始输入特征经过嵌入层后得到深度输入特征$\textit{\textbf{H}} = \{\textit{\textbf{h}}^n_t\}_{n=1,t=1}^{N,T_o}$。其中，$\textit{\textbf{h}}^n_t\in{\mathbb{R}^{d_m}}$，$d_m$为深度特征维度。由于时空Transformer编码器中的时间卷积子层提取的是相邻时刻的深度特征，其中隐含了时间信息。因此，原始输入特征并未经过位置编码层进行编码。

\paragraph{空间注意力子层} 
空间注意力子层可以看作是提取时空图中的空间边子集$\textit{\textbf{E}}_S$的特征。整个空间注意力子层的过程可以看成特殊的消息传递方式。对于场景历史时刻$t$的任意节点$n$，查询矩阵$\textit{{Q}}^n_t\in{\mathbb{R}^{N_h \times d_q}}$，键矩阵$\textit{{K}}^n_t\in{\mathbb{R}^{N_h \times d_k}}$和值矩阵$\textit{{V}}^n_t\in{\mathbb{R}^{N_h \times d_v}}$通过分别独立计算深度输入特征$\textit{\textbf{h}}^n_t\in{\mathbb{R}^{d_m}}$的线性投影得到。
\begin{equation}
\begin{split}
    \textit{{Q}}^n_t &=\{q\}^{n,N_h}_{t,i=0}, \ q^{n}_{t,i} =W_q\cdot{\textit{\textbf{h}}^n_t}\\
    \textit{{K}}^n_t &=\{k\}^{n,N_h}_{t,i}, \ k^{n}_{t,i} =W_k\cdot{\textit{\textbf{h}}^n_t}\\
    \textit{{V}}^n_t &=\{v\}^{n,N_h}_{t,i}, \ v^{n}_{t,i} =W_v\cdot{\textit{\textbf{h}}^n_t}
\end{split}
\end{equation}
式中：$W_q\in{\mathbb{R}^{{d_m}\times{d_q}}}$；$W_k\in{\mathbb{R}^{{d_m}\times{d_k}}}$；$W_v\in{\mathbb{R }^{{d_m}\times{d_v}}}$。

% 在得到所有空间节点的查询向量、键向量和值向量后，
在任意两个节点$n$和节点$j$之间，通过并行计算$\textit{{Q}}^n_t$和$\textit{{K}}^j_t$两矩阵之间每组向量的缩放点积可以获得节点\textit{j}相对于节点\textit{n}的注意力得分作为从节点$j$传递到节点$n$的消息，如图\ref{fig:3_spt}所示。第\textit{i}组消息可以表示为：
\begin{equation}
    \textit{\textbf{m}}_{t,i}^{nj} = \textit{\textbf{q}}^n_{t,i} \cdot {\textit{\textbf{k}}^j_{t,i}}^T
\end{equation}

\begin{figure}[H]
\centering
\includegraphics[height=5.8cm]{3_spt.pdf}
\caption{空间注意力子层的消息传递示意图}
\label{fig:3_spt}
\end{figure}

从包括自身的所有节点发送到节点$n$的消息经过归一化以及求和操作后获得节点$n$的注意力头$head^n_{t,i}$。通过$N_h$个并行的消息提取过程形成多个注意力头后，对这些注意力头执行级联操作并通过全连接层网络得到空间注意力子层输出的空间深度特征$MultiHead(Q^n_t,K^n_t,V^n_t)$。消息提取过程和多头注意力生成过程可以描述为：
\begin{equation}
\begin{split}
        head^n_{t,i} &= \sum_{j} Softmax\frac{\textit{\textbf{m}}_{t,i}^{nj}}{\sqrt{d_k}}v_{t,i}^j, \\
        MultiHead(Q^n_t,K^n_t,V^n_t) &= \textit{\textbf{W}}_o\cdot Concat(head_{t,0}^{n},\cdots,head_{t,N_h}^n)
\end{split}
\end{equation}


\paragraph{时间卷积子层}
从空间注意力子层获得空间特征信息后，时空Transformer编码器对时空图中的时间边集合应用时间卷积操作，从而对同一交通参与者轨迹序列内的时间动态特征进行提取。时间卷积子层使用核大小为$K\times{1}$的标准二维卷积沿时间维度对给定形状为$(T_o,N,d_m)$的深度输入特征进行特征提取。其中，$T_o$是历史帧长度，\textit{N}是空间节点数，$d_m$是特征数。
\begin{equation}
    \Bar{\textit{\textbf{h}}}^n=TCN_{K\times{1}}(\{MultiHead(Q^n_t,K^n_t,V^n_t)\}_{t=1}^{T_o})
\end{equation}
式中：\textit{TCN}表示时间卷积子层，$\Bar{\textit{\textbf{h}}}^n$为时间卷积子层的输出特征。

时间卷积子层能够对同一交通参与者的时间特征进行有效提取，并且与空间注意力子层交替使用从而得到融合的时空特征。需要说明的是，S2TNet网络从效率和精度两方面综合考虑后选用了时间卷积子层，而非时间注意力子层。

\xsubsection{时间Transformer编码-解码器}{Temporal Transformer Encoder and Decoder}
在时空Transformer编码器后，S2TNet网络引入了时间Transformer编码器，从而更好地提取出交通参与者在时间维度上的动态特征。时间Transformer解码器以自回归的形式将编码器的深度特征和先前的预测轨迹结合，生成当前预测时刻轨迹。

\paragraph{时间Transformer编码器}
时间Transformer编码器由$N_h=8$组相同的编码器网络层组成。每层由时间注意力子层和可分离卷积子层组成。时间注意力子层类似于空间Transformer编码器中的空间注意力子层，不同之处在于时间注意力子层是为每个交通参与者独立计算沿时间维度的相关性，如图\ref{fig:3_tem}所示。

\begin{figure}[H]
\centering
\includegraphics[height=5.8cm]{3_tem.pdf}
\caption{时间注意力子层示意图}
\label{fig:3_tem}
\end{figure}

与第2章\ref{tf_encoder}小节中的多头注意力子层的提取过程相同，交通参与者\textit{n}的时间多头注意力可以表示为：
\begin{equation}
    MultiHead(Q'^n,K'^n,V'^n) = \textit{\textbf{W}}_u \cdot Concat(head'^n_0,\cdots,head'^n_{N_h}) \label{mmmm}
\end{equation}
式(\ref{mmmm})中的每个注意力头可以表示为：
\begin{equation}    
    {head}'_i = Softmax\frac{Q'^n \cdot (K'^n)^T}{\sqrt{d_k}}V'^n
\end{equation}
式中：$Q'^n$、$K'^n$和$V'^n$是从交通参与者$n$从时空Transformer编码器输出的深度特征$\Bar{\textbf{\textit{h}}}^n$中独立学习到的查询矩阵、键矩阵和值矩阵。

与第2章Transformer编码器使用的多层前馈网络子层不同，时间Transformer编码器使用可分离卷积子层\cite{Chollet16a}作为在更高维度上提取时间特征的手段。可分离卷积（Separable Convolution）的核心思路是依次使用逐深度卷积和逐点卷积的方式，将完整的卷积步骤分解。可分离卷积子层的好处是能够降低运算量，并获得更高的性能。交通参与者\textit{n}的多头注意力子层输出$MultiHead(Q'^n,K'^n,V'^n)$经过可分离卷积子层的深度特征提取后，更新为：
\begin{equation}
    \overline{\textbf{\textit{h}}}^n = SeparableConv(MultiHead(Q'^n,K'^n,V'^n))
\end{equation}

\paragraph{时间Transformer解码器}
时间Transformer解码器的输入是$\overline{\textbf{\textit{H}}}=\{\overline{\textbf{\textit{h}}}^n\}_{n=1}^N$和前一时刻预测出的所有交通参与者的轨迹位置。在输入时间Transformer解码器之前，前一时刻预测出的轨迹位置需要经过嵌入层将特征映射成深度特征，然后将时间的相对位置信息以位置编码的形式加入深度特征中。

与第2章\ref{tf_encoder}小节解码器相同，S2TNet网络的时间Transformer解码器由$N_h=8$组相同的编码器网络层组成。每层由时间掩码注意力子层、时间注意力子层和可分离卷积子层组成。首先，解码器采用具有掩码机制的注意力子层来确保$T_o+t$时刻的预测只能依赖于小于$T_o+t$时刻的输出轨迹位置。然后，时间注意力子层将时间掩码注意力子层输出$MaskMultiHead(Q'', K'', V'')$映射成查询矩阵，在时间Transformer编码器的输出$\overline{\textbf{\textit{H}}}$映射出的键矩阵和值矩阵上执行了多头注意力操作，得到$MultiHead(Q''', K''', V''')$。最后，$MultiHead(Q''', K''', V''')$依次经过可分离卷积子层和全连接层网络的处理后得到预测时刻$T_o+t$的轨迹位置。以交通参与者\textit{n}为例，整个过程可以表示为：
\begin{equation}
\begin{split}
    MaskMultiHead(Q''^n, K''^n, V''^n) &= \textit{\textbf{W}}_{de1}\cdot Concat(\{head''^n_i\}_{i=1}^{N_h}), \\
    MultiHead(Q'''^n, K'''^n, V'''^n) &= \textit{\textbf{W}}_{de2}\cdot Concat(\{head'''^n_i\}_{i=1}^{N_h}), \\
    \overline{\textbf{\textit{h}}}'^n &= SeparableConv(MultiHead(Q'''^n, K'''^n, V'''^n)), \\
    u^n_{T_o+t} &= \textit{\textbf{W}}_{out}(\overline{\textbf{\textit{h}}}'^n) \\
\end{split}
\end{equation}
式中：$MaskMultiHead(Q'', K'', V'')$为时间掩码注意力子层的输出；$MultiHead(Q''', K''', V''')$为时间注意力子层的输出；$\overline{\textbf{\textit{h}}}'^n$为可分离卷积子层的输出；$u^n_{T_o+t}$为最终轨迹输出。


\xsubsection{实现细节}{Implementation Details}
整个S2TNet网络是采用PyTorch框架实现的。在数据处理阶段，训练集数据使用随机旋转的方式进行数据增强。深度特征的维度设置为$d_m = 32$。时间Transformer解码器残差连接步骤之前每个子层的输出以及时间Transformer解码器的位置编码输出特征都应用了Dropout操作，丢弃率设置为0.1。

训练时，输出轨迹与真实轨迹之间采用$L_2$损失函数。
\begin{equation}
    Loss=\sum^{T_o+T_f}_{t=T_o+1}{|\hat{\textbf{\textit{Y}}} - \textbf{\textit{Y}}|^2}
\end{equation}
式中：$\hat{\textbf{\textit{Y}}}$和\textbf{\textit{Y}}分别是预测轨迹和未来轨迹真值。

PPNet网络使用Adam\cite{kingma2014adam}作为优化器，并采用如下学习率变化策略：
\begin{equation}
    learning\_rate=d^{-0.5}_{d_m}\cdot{min(step\_num^{-0.5},step\_num\cdot{warmu\_steps^{-1.5}})}
\end{equation}
式中：$warmup\_step$设置为5000。

\xsection{实验结果与分析}{Experimental Results and Analysis}
\xsubsection{数据集和评价标准}{Dataset and Evaluation Metrics}
\paragraph{ApolloScape轨迹数据集}
S2TNet网络在Apollo自动驾驶汽车收集的ApolloScape轨迹数据集\cite{ma2019trafficpredict}上进行了深入的评估。ApolloScape轨迹数据集提供图像、点云和手工标注的轨迹。该轨迹集是在中国北京以不同照明和交通密度条件收集的。ApolloScape轨迹数据集能够提供极其复杂的城市交通流，包括车辆、非机动车和行人的大量交互。该数据集分为53分钟的训练数据集和50分钟的测试数据集。数据采样频率是2赫兹。ApolloScape轨迹数据集官方设定的预测要求是根据历史的6帧轨迹数据来预测未来的6帧轨迹数据。ApolloScape轨迹数据集并不公开提供其测试集的未来真实轨迹，需要通过上传到ApolloScape官网上获得测试集的预测结果。因此，本章的所有定量结果都是通过上传ApolloScape官网的方式得到的公开测试结果。

\paragraph{评价指标}
与常用的ADE和FDE不同，本章采用了ApolloScape轨迹数据集官方设定的两个评价标准，包括加权平均位移误差（Weighted Sum of ADE， WSADE）和加权最终位移误差(Weighted Sum of FDE，WSFDE)。其中，加权平均位移误差是所有不同类别的交通参与者按照预先设定的类别权值计算出预测轨迹和真实轨迹的平均欧氏距离，加权最终位移误差是按照相同类别权值计算出的最后时刻预测轨迹和真实轨迹的欧氏距离。加权平均位移误差能够体现出算法的平均预测性能，而加权最终位移误差能够反映长期预测的精度。
\begin{equation}
\begin{split}
    WSADE &=D_v\cdot{ADE_v}+D_p\cdot{ADE_p}+D_b\cdot{ADE_b},\\
    WSFDE &=D_v\cdot{FDE_v}+D_p\cdot{FDE_p}+D_b\cdot{FDE_b}  
\end{split}
\end{equation}
式中：$D_v=0.20$、$D_p=0.58$和$D_b=0.22$是数据集中对应车辆、行人和非机动车辆的权值。

\xsubsection{基准方法介绍}{Baselines}
为了对性能进行全面的评估，S2TNet网络与大量的基准方法进行了比较，主要包括：
\begin{itemize}
    \item Constant Velocity（CV）：使用历史轨迹的平均速度作为未来恒定的运动速度进行轨迹预测。
    \item TP\cite{ma2019trafficpredict}(TrafficPredict)：根据不同交通参与者的类别，使用多个不同的LSTM按照类别共性特征进行轨迹预测。
    \item StarNet\cite{DBLP:journals/corr/abs-1906-01797}：构建了一个星形拓扑来考虑所有交通参与者之间的影响。
    \item Social-LSTM (SLSTM)\cite{slstm}：首先使用LSTM提取单个交通参与者的时间特征，然后采用社交池机制融合空间邻域内的交互信息。
    \item Social-GAN(SGAN)\cite{sgan}：通过序列预测和生成对抗性网络相结合进行轨迹预测。同时，该模型使用了一种新的池化机制来聚合社会交互信息。
    \item TF\cite{transformer}(Transformer)：不考虑任何复杂的空间交互或场景交互，仅使用原始的时间Transformer网络对交通参与者进行特征提取。
    \item STAR\cite{star}：通过交错使用空间Transformer和时间Transformer以提取交通参与者之间的时空交互特征。
    \item TPNet\cite{tpnet}：首先根据场景信息生成一组候选的未来轨迹，然后通过对候选者进行分类和提炼得到最终的预测。
    \item GRIP++\cite{grip++}：是之前性能最好的方法。GRIP++使用固定图和动态图两种类型的增强图表示限定邻域内交通参与者的交互作用，并应用时空图卷积网络有效提取了时空特征。
\end{itemize}

\xsubsection{定量结果与分析}{Quantitative Results and Analyses}
表\ref{3_tab1}展示了S2TNet网络与所有基准方法的比较结果。从表\ref{3_tab1}可以看出S2TNet网络的性能优于所有的基准方法。与之前性能最好的方法GRIP++相比，S2TNet网络在车辆、行人和非机动车辆三个类别上，ADE分别降低了11.28$\%$、4.31$\%$和10.24$\%$，FDE分别降低了12.21$\%$、4.98$\%$和5.87$\%$。由此可知，S2TNet网络在车辆和非机动车辆上的性能提高程度明显优于行人。其原因是行人的运动模式比具有非完整约束的车辆和自行车更加灵活。值得注意的是，仅利用历史轨迹平均速度的匀速模型方法优于许多深度学习方法，包括在行人轨迹预测方面取得高性能的方法STAR。这表明针对同种类别的轨迹预测方法可能无法有效地处理含有密集异质交通参与者的城市场景。相反，S2TNet网络在异质交通参与者的城市密集环境中表现更好。可视化结果能够进一步证明这一点。

\AtBeginEnvironment{tabularx}{\xiaowu}
\begin{table}[H]
\caption{S2TNet网络与基准方法在ApolloScape轨迹数据集上的性能对比}
\label{3_tab1}
\begin{tabularx}{\textwidth}{*{9}Y}
\toprule
\multirow{2}{*}{方法}& \multicolumn{8}{c}{性能/米}  \\ \cline{2-9}
         &WSADE           &ADEv  &ADEp   &ADEb   &WSFDE               &FDEv  &FDEp   &FDEb    \\ \midrule
TP\cite{ma2019trafficpredict} & 8.5881          & 7.9467 & 7.1811 & 12.8805 & 24.2262         & 12.7757 & 11.121 & 22.7912 \\ 
SLSTM\cite{slstm}         & 1.8922          & 2.9456 & 1.2856 & 2.5337  & 3.4024          & 5.2802  & 2.3240 & 4.5384  \\ 
SGAN\cite{sgan}          & 1.5829          & 3.0430 & 0.9836 & 1.8354  & 2.7796          & 5.0913  & 1.7264 & 3.4547  \\ 
STAR\cite{star}           & 1.5400          & 2.5644 & 0.9473 & 2.1714  & 2.8602          & 4.6324  & 1.8029 & 4.0366  \\ 
CV             & 1.4762          & 2.6454 & 0.8547 & 2.0519  & 2.7601          & 4.7944  & 1.6428 & 3.8564  \\ 
StraNet\cite{DBLP:journals/corr/abs-1906-01797}        & 1.3425          & 2.3860 & 0.7854 & 1.8628  & 2.4984          & 4.2857  & 1.5156 & 3.4645  \\ 
TF\cite{transformer}    & 1.2803          & 2.2322 & 0.7398 & 1.8398  & 2.4024          & 4.0317  & 1.4309 & 3.4826  \\ 
TPNet\cite{tpnet}          & 1.2800          & 2.2100 & 0.7400 & 1.8500  & 2.3400          & 3.8600  & 1.4100 & 3.4000  \\ 
GRIP++\cite{grip++}        & 1.2588          & 2.2400 & 0.7142 & 1.8024  & 2.3631          & 4.0762  & 1.3732 & 3.4155  \\ \midrule[0.5pt] 

S2TNet   & {\bf 1.1679} & {\bf 1.9874} & {\bf0.6834} & \bf{1.7000}  & {\bf 2.1798} & {\bf 3.5783}  & {\bf 1.3048} & {\bf 3.2151}  \\ 
$\Delta{GRIP}$++ &7.22\% &11.28\% &4.31\% &10.24\% &7.76\% &4.98\% &12.21\% &5.87\% \\ 
\bottomrule
\end{tabularx}
\end{table}
\AtBeginEnvironment{tabularx}{\wuhao}

\xsubsection{可视化结果与分析}{Qualitative Results and Analyses} 

为了直观地分析不同轨迹预测方法的性能，图\ref{fig:3_5}展示了S2TNet网络和GRIP++在完整的自动驾驶场景中的可视化对比。

\begin{figure}[H]
\centering
\includegraphics[height=8.55cm]{3_5.pdf}
\caption{自动驾驶全场景预测结果可视化对比图}
\label{fig:3_5}
\end{figure}

从图\ref{fig:3_5}可以看出，S2TNet网络能够准确地提取时空交互特征。在图\ref{fig:3_5}两个子图的右侧部分，车辆与未知类别的交通参与者沿相反方向行驶。可以看出，GRIP++未能准确理解两者的交互关系，导致其预测结果偏离了真实轨迹，而S2TNet网络则准确地预测出了两者的行驶路线。同时，S2TNet网络能够成功预测出静止交通参与者的轨迹。在图\ref{fig:3_5}左侧部分，两辆车在减速后达到静止的状态。GRIP++预测的车辆静止位置与真实静止位置偏差较大。相反，S2TNet网络则准确预测出了静止时刻和位置。

图\ref{fig:3_single}分别展示了S2TNet网络与GRIP++在ApolloScape轨迹数据集上不同类别的交通参与者的可视化结果。需要说明的是，这些结果都是从密集的交通流中筛选出来的。

\begin{figure}[H]
\begin{subfigure}[b]{0.49\linewidth}
\centering
\includegraphics[height=5.8cm]{3_a.pdf}
\subcaption{车辆}
\label{3_a}
\end{subfigure}
\begin{subfigure}[b]{0.49\linewidth}
\centering
\includegraphics[height=5.8cm]{3_b.pdf}
\subcaption{行人}
\label{3_b}
\end{subfigure}

% \floatcontinue{tb}

\begin{subfigure}[b]{0.49\linewidth}
\centering
\includegraphics[height=5.8cm]{3_c.pdf}
\subcaption{非机动车辆}
\label{3_c}
\end{subfigure}
\begin{subfigure}[b]{0.49\linewidth}
\centering
\includegraphics[height=5.8cm]{3_d.pdf}
\subcaption{其他类别}
\label{3_d}
\end{subfigure}
\caption{不同类别交通参与者预测结果可视化对比图}
\label{fig:3_single}
\end{figure}

从图\ref{fig:3_single}中可以看出，S2TNet网络能够基于6帧（3秒）的历史轨迹预测出不同类别交通参与者的未来3秒内轨迹。在图\ref{3_a}中，车辆正在执行转弯操作。S2TNet网络输出的轨迹能够使车辆继续沿着预定路线行驶，而GRIP++预测的结果则会使车辆偏离预定轨迹。在图\ref{3_b}中，行人正在大致沿历史轨迹方向行走，GRIP++预测的行人轨迹会发生转向，导致长期轨迹与真实轨迹偏差很大，而S2TNet网络预测的轨迹整体方向是非常准确的。在图\ref{3_c}中，GRIP++预测的轨迹速度明显低于真实轨迹，而S2TNet网络预测的轨迹速度与真实轨迹的速度非常吻合。在图\ref{3_d}中，S2TNet网络的累积误差也明显小于GRIP++。

\xsubsection{消融实验}{Ablation Study}
本节对S2TNet网络进行消融实验研究，以充分验证S2TNet网络各个模块的有效性。模块包括时空Transformer编码器中的空间注意力子层（SS）和时间卷积层（TCN），时间Transformer编码器（TE）和时间Transformer解码器（TD）所使用的可分离卷积子层（SC）。同时，将可分离卷积子层与原始Transformer中使用的多层前馈神经网络（FC）的效果进行了对比。针对历史特征（HF）的影响，消融实验对所有可用的历史特征信息（A）与仅使用全局坐标（C）的输入特征之间的效果进行了对比。所有可用的历史特征包括全局坐标、类别、长度、宽度和航向角。消融实验针对空间注意力子层中空间限制范围（LM）的影响，使用一定空间限制范围内的空间特征（N）替换没有空间范围限制（W）的空间特征，并对其结果进行对比。设定的空间限制范围是自动驾驶车辆周围15米范围内的所有交通参与者。消融实验结果显示在表\ref{3_tab2}中。

% \AtBeginEnvironment{tabularx}{\xiaowu}
% \AtBeginEnvironment{tabularx}{\wuhao}
\begin{table}[H]
\setlength{\abovecaptionskip}{-6pt}
% \setlength{\belowcaptionskip}{0pt}
\caption{消融实验结果}

\centering
\begin{center}
\begin{tabularx}{\textwidth}{*{8}Y}
\toprule
\multirow{2}*{ID} &\multicolumn{6}{c}{成分}  & 性能/米   \\ \cline{2-7}
    & SS            & TCN            & TE       & TD    & HF     & LM       & (ADE/FDE) \\ \midrule
1 &$\times$       &$\times$        & SC       & SC    & A      &W   & 1.23/2.29 \\
2 &$\times$       & \checkmark     & SC       & SC    & A      &W       & 1.22/2.26              \\
3 &\checkmark     &$\times$        & SC       & SC    & A      &W       & 1.25/2.36              \\
4 &\checkmark     &\checkmark      &$\times$  & SC    & A      &W       & 1.27/2.41 \\
5 &\checkmark     &\checkmark      & FC       & FC    & A      &W       & 1.19/2.26 \\
6 &\checkmark     &\checkmark      & SC       & SC    & C    &W       & 1.22/2.30 \\
7 &\checkmark     &\checkmark      & SC       & SC    & A     &N       & 1.27/2.35 \\ \midrule[0.5pt]
8 &\checkmark     &\checkmark      & SC       & SC    & A      &W       & \textbf{1.17/2.18} \\ 
\bottomrule
\end{tabularx}
\label{3_tab2}
\end{center}
\end{table}
% \AtBeginEnvironment{tabularx}{\wuhao}

从表中可以看出：
\begin{itemize}
     \item {时空Transformer编码器可以充分地提取到空间和时间维度上的特征信息}。ID1舍弃了整个时空Transformer编码器，仅采用时间Transformer编码-解码结构进行轨迹预测，其性能与ID8使用整个S2TNet网络比较有明显的下降。与ID1相比，包含时间卷积子层的ID2优于单独使用时间Transformer结构。与ID1相比，包含空间注意力子层和时间Transformer编码器的ID3在验证集中表现出色，但是在测试集中的性能较差。这是因为仅将注意力集中在空间维度上而不融合时间信息会导致过拟合。
     \item {时间Transformer编码器增强了对时间序列数据的特征提取能力}。ID4删除了时间Transformer编码器，与ID8相比性能较低。这表明时间注意力机制可以有效地提取到时间信息。
     \item {在时间Transformer编码-解码器中的可分离卷积子层性能优于多层前馈神经网络}。ID5将时间Transformer编码-解码器中的可分离卷积子层替换为全连接前馈网络，性能略有下降。
     \item {输入更多有效特征能够获得更高的准确度}。ID6中仅输入历史轨迹，而不是所有可用的有效特征。与ID8对比可知，丰富的输入特征信息有助于网络理解交通参与者的异质性。
     \item {提取全场景的空间注意力优于仅提取空间限制内的空间注意力}。ID7中使用了掩蔽机制忽略了设定空间限制（15米）外的空间交互影响\cite{grip++}。与ID8对比可知，整个场景中的交通参与者对轨迹预测的准确性都有很大影响，而非一定邻域内的部分交通参与者。
\end{itemize}

\xsubsection{研究平台部署}{Deployment in Research Platform}
本节介绍S2TNet网络在CARLA仿真平台和“发现号”实际平台的部署情况。在仿真平台上，S2TNet网络能够直接获取到当前场景内所有目标的准备准确感知结果作为其预测输入。在“发现号”实际平台上，不同类型的交通参与者需要经过多传感融合的目标检测、识别和跟踪算法后得到带有标签信息的感知结果作为“发现号”预测模块的输入。“发现号”预测模块的设置与ApolloScape轨迹数据集相同，即基于6帧（3秒）的历史轨迹预测出不同类别交通参与者的未来6帧的确定性轨迹。为了实现该设置，S2TNet网络采用的基础数据结构是最近最久未使用法（Least Recently Used，LRU）。该数据结构可以设定缓存淘汰机制，每次从内存中找到最久未使用的数据然后置换出来。因此，可以利用LRU实现存取交通参与者连续6帧历史轨迹的功能。在实际工程设计中，LRU利用双向链表实现。在定义好头节点和尾节点后，可以采用先进先出(First In First Out，FIFO)原则，执行添加、访问、修改和删除四项操作获取被放入的数据。

% 添加是将新元素直接放在链表头上面，其他的元素顺序往下移动；访问是将中间位置或者末尾的数据移动到头节点；修改操作在修改原值之后也将数据移动到头部；执行删除数据操作后，其他元素按顺序向前移动。
S2TNet网络在CARLA仿真平台不同视角下的预测效果如图\ref{fig:carla}所示。其中蓝色车辆为自动驾驶汽车，红线和蓝线分别为车辆和行为的预测结果。

\begin{figure}[H]
\begin{subfigure}[b]{0.49\linewidth}
\centering
\includegraphics[height=5.8cm]{3_carlaa.pdf}
\subcaption{俯视图}
\end{subfigure}
\begin{subfigure}[b]{0.49\linewidth}
\centering
\includegraphics[height=5.8cm]{3_carlab.pdf}
\subcaption{侧视图}
\end{subfigure}
\caption{S2TNet网络在CARLA仿真平台的预测效果}
\label{fig:carla}
\end{figure}

从图中可以看出，S2TNet网络能够成功预测自动驾驶车辆行驶至繁忙交通路口时周围不同类型交通参与者的未来轨迹。

S2TNet网络部署在“发现号”研究平台上连续4秒（$T=1-4s$）内的实际预测效果如图\ref{fig:discovery}所示。从图中可以看出，S2TNet网络能够对车道上正常行驶的车辆以及十字路口转弯的车辆进行预测。
% 从图中可以看出，因为实际平台中存在误检现象，如何在包含误检和漏检的真是检测环境中进行预测。

\begin{figure}[H]
\begin{subfigure}[b]{0.49\linewidth}
\centering
\includegraphics[height=5.8cm]{3_d1.pdf}
\subcaption{T=1s}
\end{subfigure}
\begin{subfigure}[b]{0.49\linewidth}
\centering
\includegraphics[height=5.8cm]{3_d2.pdf}
\subcaption{T=2s}
\end{subfigure}
\begin{subfigure}[b]{0.49\linewidth}
\centering
\includegraphics[height=5.8cm]{3_d3.pdf}
\subcaption{T=3s}
\end{subfigure}
\begin{subfigure}[b]{0.49\linewidth}
\centering
\includegraphics[height=5.8cm]{3_d4.pdf}
\subcaption{T=4s}
\end{subfigure}
\caption{S2TNet网络在“发现号”平台的预测效果}
\label{fig:discovery}
\end{figure}

\xsubsection{本章小结}{Chapter Summary}
本章提出了基于Transformer的S2TNet网络，可以用于预测自动驾驶汽车周围不同类别交通参与者的单模态轨迹。时空Transformer编码器能够提取所有交通参与者之间的时空交互，而不仅限于空间邻域内的部分。时间Transformer编码-解码器用于进一步生成时间表征，并以自回归的方式预测出未来轨迹。在ApolloScape轨迹数据集上的实验结果表明，S2TNet网络显著提高了单模态轨迹预测的准确性。同时，S2TNet网络也成功部署在了仿真和实车平台中。