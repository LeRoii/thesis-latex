% !TeX root = ../main.tex
\xchapter{基于双重注意力处理的高效可见光航拍图像小目标检测算法}{Efficient Drone Object Detection Network Based on Bipartite Attentive Processing}

无人机航拍图像中的目标检测面临着一系列严峻挑战，包括极端尺度变化、密集分布的小目标以及复杂的背景，这导致通用目标检测器在此类场景下性能明显下降。最直接的解决方案是提升输入图像的分辨率，但这会急剧增加计算负担。现有方法由于在网络架构上存在缺陷，难以在保持对小目标检测至关重要的细粒度特征的同时，实现精度与速度的平衡。为此，本章节设计了一种基于RT-DETR框架的优化模型架构，提出了基于通道分离的双重注意力处理模块，通过通道分离策略，实现了卷积操作与注意力机制并行处理，从而增强了模型从复杂航拍图像中提取判别性特征的能力，同时引入频率感知融合模块，能够有效保留关键的低层细节特征，并将其与高层语义信息深度融合。

\xsection{引言}{Introduction}
作为无人机的核心感知单元，机载智能光电系统在对地观测、目标识别与态势感知等关键任务中发挥重要作用。目标检测算法使无人机能够识别并定位图像中的物体，从而增强其自主环境感知能力。随着卷积神经网络与视觉Transformer的快速发展，通用目标检测器在MS COCO\cite{Lin2014COCO}等通用图像数据集上取得了巨大进步。然而，面对航拍图像时，通用检测器的性能出现明显下降。例如，当前最受欢迎的基于CNN的检测器YOLOv11-M \cite{Jocher2023UltralyticsYOLO}，在MS COCO数据集上的$AP_{50}$为51.5\%，但在VisDrone \cite{Cao2021VisDroneDET2021}数据集上仅为43.1\%，基于Transformer的端到端检测器RT-DETRv2-S \cite{Lv2024RtDETRv2}在MS COCO上$AP_{50}$为63.8\%，在VisDrone上则为45.3\%。这一明显的性能差距表明，无人机视角下的目标检测技术仍需改进。

与常规图像相比，由于无人机拍摄高度和角度的变化，航拍图像中的目标通常表现出以下三个特征\cite{Leng2024AerialObjectSurvey}：1) 小目标占比极高；2) 小目标往往在特定区域聚集；3) 同类目标的尺度变化极为剧烈。如图\ref{fig1}所示，VisDrone数据集中大多数目标的尺寸小于20像素。密集区域中的目标可能相互重叠与遮挡，导致漏检与误检。这些因素严重影响了检测性能，并制约了无人机自主感知系统的可靠性。

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{visdrone-distribute-a.png} 
        \caption{}
        \label{fig1:a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{visdrone-distribute-b.png}
        \caption{}
        \label{fig1:b}
    \end{subfigure}
    \caption{(a)VisDrone数据集中目标的平均尺寸分布，其中，横轴的目标尺寸由边界框宽度与高度乘积的平方根计算得出。如图所示，绝大多数目标的尺寸集中在20像素以内。(b)小目标在特定区域呈现不均匀密集分布}
    \label{fig1}
\end{figure}


检测密集分布的小目标是一项极具挑战的任务，这主要源于其有限的有效像素难以提供具有差异化的视觉特征，且在高密度聚集时易产生相互遮挡与背景混淆，导致传统检测方法难以实现稳定、可靠的识别与定位。常见解决方案包括多尺度特征融合\cite{sniper2018,Singh2017SNIP}、数据增强\cite{kisantal2019augmentation}、背景建模\cite{Li2025RethinkingSparseMask}和聚焦检测\cite{Yang2019ClusteredOD}。然而，这些方法通常难以解决密集小目标差异化特征表征不足的问题，而直接提升图像分辨率或进行裁剪的方法则会带来难以承受的计算开销，且计算常浪费在背景区域。此外，基于CNN的检测器存在超参数敏感性问题，在目标重叠区域容易因非极大值抑制而产生误差\cite{Tan2024UAVAerialDetection}。对于航拍图像，极端尺度变化和复杂背景加剧了这些问题。尽管如DETR\cite{Carion2020DETR}等端到端检测器通过集合预测抛弃了NMS和锚框，但其对航拍场景下小目标的适应能力仍然不足。DQ-DETR\cite{Huang2024DQDETR}作为首个专为小目标检测设计的类DETR模型，虽提升了精度，却以收敛速度慢和高计算量为代价。RT-DETR\cite{Lv2024RtDETRv2}通过混合编码加速推理，但其架构面对密集小目标检测仍存在局限。通过分析其具体架构，我们发现其主干网络缺乏针对小目标细粒度特征的专门优化，且混合编码器仅利用高层特征，此时小目标已历经多次下采样，导致对检测至关重要的空间与纹理信息严重流失。因此，在无人机目标检测任务中同时实现实时速度与高精度仍是一个挑战。

基于上述分析，本章提出一种基于实时端到端RT-DETR框架的优化架构。目标在于针对性解决下列核心挑战：1) 增强差异化特征学习：利用新设计的双重注意力处理块重构主干网络，以生成更丰富、更具判别力的特征表示；2) 优化多尺度融合与小目标特征保留：提出了双融合特征编码器，显式结合高分辨率特征以保留对小目标至关重要的细节。此外，采用频率感知融合模块来整合跨尺度特征，增强上下文建模与特征兼容性；3) 优化小目标相似性度量：将倒数归一化Wasserstein距离与CIoU相结合构建损失函数，提供了与尺度无关的边界框相似性度量，提升了密集小目标的定位精度。大量实验证实，我们的方法为航拍图像检测建立了新的效率-精度均衡标准。

本章的主要贡献总结如下：
\begin{itemize}
\item 在主干网络中引入了双重注意力处理模块。该模块将注意力机制与并行分支处理相结合，超越了简单的“分割-处理-合并”模式，确保了更具交互性的特征提取与注意力建模。
\item 设计了双融合特征编码器，有效保留小目标的特征并改善多尺度特征融合。具体包括引入低层特征图、重构融合路径以及设计频率感知融合模块。
\item 提出了RNWD-CIoU损失函数，该混合损失函数利用经过倒数归一化的Wasserstein距离，为边界框相似性提供了与尺度无关的度量方式，在不引入任何额外推理计算开销的情况下，提升了密集小目标的检测性能。
\item 在三个公开航拍图像数据集（Visdrone\cite{Cao2021VisDroneDET2021}，UAVDT\cite{Du2018UAVDT}，AI-TOD\cite{Wang2021AITOD}）上的大量实验表明，与近年出现的各类先进算法相比，BAP-DETR以最小的计算负载和参数量，实现了最优性能，特别地，在VisDrone数据集上，BAP-DETR的$AP$提升了6.9\%，同时减少了17.5\%的计算量。
\end{itemize}

\xsection{相关工作}{Related Work}
\xsubsection{基于CNN的小目标检测方法}{CNN-based Small Object Detection Methods}
与通用数据集中的图像相比，航拍图像中的目标检测通常更具挑战性。这主要因为图像中存在数量更多的小目标，其有限的像素使得特征提取极为困难，且在卷积过程中，小目标的特征容易被背景或其他目标干扰。此外，这些小目标在图像中分布不均，多数呈密集聚集状态，这不仅使精确定位更为困难，也导致了更高的漏检率。

为应对小目标检测难题，研究者们提出了多种解决方案，主要包括样本导向\cite{kisantal2019augmentation}、基于注意力\cite{Lu2021AttentionFFSSD}、多尺度融合\cite{Singh2017SNIP}及聚焦检测\cite{Yang2019ClusteredOD,Xu2025DynamicAnchor}等方法。这些方法的核心大多围绕如何解决因像素有限导致的低质量特征表示问题。然而，在存在极端尺度变化和密集目标的航拍场景中，这些通用方法的性能仍显不足。样本导向方法常面临性能提升不稳定和迁移性差的问题，基于注意力的方法虽凭借其灵活的嵌入设计而备受推崇，可便捷地集成到各类架构中，但其性能提升往往以复杂的相关运算所带来的沉重计算开销为代价，多尺度融合架构期望以合适的尺度处理小目标，但不同尺度特征融合时易引入噪声或冲突，可能导致小目标的特征在融合过程中被淹没或扭曲。

针对航拍图像中密集小目标的特定挑战，一些研究提出了专门设计。例如，QueryDet\cite{Yang2022QueryDet}设计了级联查询策略，以避免在低层特征上的冗余计算，从而能高效地在高分辨率特征图上检测小目标，但其精度依赖于初始预测的准确性。DMNet\cite{Li2020DMNet}和Dynamic Anchor\cite{Xu2025DynamicAnchor}等方法利用密度图来检测目标并学习尺度信息。CEASC\cite{Du2023CEASC}将全局上下文集成到稀疏卷积网络中，以增强无人机图像的目标检测。SAHI\cite{Akyon2022SAHI}采用了裁剪策略，虽提升了精度，但也增加了计算复杂度和处理时间，其均匀裁剪方法未能考虑目标分布的非均匀性，导致检测所有图像块耗时巨大，效率降低。ClusDet\cite{Yang2019ClusteredOD}利用特定模块搜索可能包含目标的聚类区域，但这类方法通常训练成本高、推理速度慢，且仍需依赖非极大值抑制进行后处理，这进一步降低了推理速度，并引入了可能影响速度和精度稳定性的超参数，尤其在处理密集小目标时更为明显。

\xsubsection{基于DETR的小目标检测方法}{DETR-based Small Object Detection Methods}
DETR\cite{Carion2020DETR}是首个基于Transformer架构的端到端目标检测模型。它通过二分图匹配机制，避免了手工锚框设计与复杂的后处理步骤，在性能上达到了与当时主流CNN检测器相当的水平，但因其训练收敛速度缓慢而备受制约。为此，一系列类DETR模型被提出以改进此问题。为加速收敛，Deformable-DETR\cite{Zhu2020DeformableDETR}引入了可变形注意力机制，使每个查询仅关注特征图上一小组关键采样点，从而有效提高了注意力计算效率与模型训练速度。DN-DETR\cite{Li2022DnDETR}采用了去噪训练，通过在输入中加入带有噪声的标注并让模型学习恢复，有效降低了二分图匹配的难度，加快了收敛速度。另一方面，为了降低模型的计算复杂度，Efficient DETR\cite{Yao2021EfficientDETR}和Sparse DETR\cite{Roh2021SparseDETR}专注于减少编码器-解码器层数或优化查询数量。DQ-DETR\cite{Huang2024DQDETR}是首个专门为小目标检测设计的类DETR模型，它通过动态调整查询数量并增强查询的位置感知能力来精准定位小目标。然而，这些方法在追求性能的同时，通常计算开销仍然较大，难以满足实时处理的需求。

RT-DETR\cite{Zhao202RTDETR}是首个在速度和精度上均超越同期传统CNN检测器的实时端到端目标检测器。其核心创新在于设计了一个高效的混合编码器，该编码器融合了基于注意力的同尺度特征交互模块与基于CNN的跨尺度特征融合模块。这一设计降低了计算成本，成功将DETR框架拓展至实时检测场景。尽管RT-DETR在通用目标检测上表现出色，但由于其对小目标及复杂航拍场景的适应性有限，在处理无人机图像时仍显不足。针对无人机场景的后续改进工作试图弥补这一差距，但各有折衷。RT-DETR-UAVs\cite{Tan2024UAVAerialDetection}优先保障实时性能，却在检测精度上有所牺牲，导致模型在复杂背景下难以准确识别小目标。UAV-DETR\cite{Zhang2025UAVDETR}引入了结合频率增强与语义对齐的多尺度特征融合模块，以应对小目标检测挑战，但频域信息的利用可能导致不同特征图间的语义与空间信息错位，进而引发误检。HCTD\cite{Xue2025HCTD}提出了一种专为无人机检测设计的混合CNN-Transformer架构，但由于其主干网络未针对小目标优化，且对低层特征关注不足，在保留密集小目标所需的高分辨率空间特征方面仍存在局限。

\xsection{基于双重注意力处理的目标检测网络}{Bipartite Attentive Processing Detection Network}
\xsubsection{整体架构}{Overall Architecture}
本章提出的网络整体结构如图\ref{bapdetr-arch}所示。该网络以实时端到端检测器RT-DETR为基础，其架构主要由三部分组成：一个经双重注意力处理模块（Bipartite Attentive Processing Block，BAPB）增强的ResNet主干网络、一个新式的双融合特征编码器以及一个带有辅助预测头的Transformer解码器。主干网络基于ResNet构建。在每个ResNet阶段的核心ResBlock中，集成了双重注意力处理模块与SE注意力层。BAPB模块能够增强ResNet主干网络的特征提取能力，其设计思想是：将输入特征图沿通道维度分割为两个独立的处理流，每个流分别进行独立的卷积处理和基于注意力的特征提取，最后将两个流的输出融合，为后续网络层生成判别性更强、上下文信息更丰富的特征图。

具体而言，主干网络最终会生成四个层级的特征图：S2、S3、S4和S5，记为 $S_i=R^{C\times H\times W}$。其中，S2、S3、S4和S5的空间尺寸分别对应输入图像的1/4、1/8、1/16和1/32。这些多尺度特征是后续处理的基础。双融合特征编码器负责集成主干网络输出的多尺度特征。在原始RT-DETR中，其高效混合编码器对高层特征图S5（蕴含最丰富的语义信息）使用了基于注意力的同尺度特征交互模块，这极大地减少了Transformer编码器层带来的计算开销，是模型实现实时处理的关键之一。然而，随着网络加深，小目标的特征响应会逐渐减弱。相反，低层特征图（如S2）虽然语义抽象程度低，但保留了更丰富的纹理、边缘等高频细节信息，这对小目标的精确定位至关重要。因此，我们的编码器同时将高层特征S5和低层特征S2作为输入。双融合特征编码器包含一个独特的特征融合模块，频率感知融合模块（Frequency-Aware Fusion module，FAFM）。在RT-DETR的原始设计中，其高效混合编码器在自顶向下和自底向上的融合路径中使用了相同的融合块。在自底向上的上采样路径中，简单的上采样操作容易导致特征边界模糊和空间信息丢失。为解决这一问题，我们利用FAFM模块构建了全新的双融合特征编码器，能够更有效地融合不同尺度的特征，特别是增强对高频细节的保留与利用，以提升对航拍图像中小目标的检测能力。

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{bapdetr-arch.png}
\caption{BAP-DETR网络架构示意图}
\label{bapdetr-arch}
\end{figure}

\xsubsection{双重注意力处理块}{Bipartite Attentive Processing Block}
无人机航拍图像中目标尺度变化、小目标密集分布和背景复杂等挑战，对ResNet中标准的残差块构成了严峻考验，导致其在此类场景下性能下降。为解决这一问题，我们提出了双重注意力处理块（Bipartite Attentive Processing Block, BAPB），该模块是一种全新的架构，通过将相互依赖的注意力机制与并行分支处理相结合，优化特征提取过程。BAPB的结构如图\ref{bapdetr-arch}所示。

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{bapb.png}
\caption{双重注意力模块示意图}
\label{bapb}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{att-blk.png}
\caption{注意力分支示意图}
\label{att-blk}
\end{figure}

输入特征图首先沿通道维度被均匀分割为两部分，分别记为$SP_0$和$SP_1$，并进入两个并行处理分支。在$SP_0$分支中，$SP_0$首先与$SP_1$进行逐元素相加，其和作为该分支的后续输入。这一操作在初始阶段即引入了跨通道的交互。两个分支随后分别独立地通过两个连续的3x3卷积层和一个批归一化层进行处理。通道分离策略将特征图解耦为不同的子成分，使每个分支能够专注于其输入特征的不同特性。最后，两个分支的处理输出沿通道维度进行拼接，生成优化后的融合特征$F_{fused}$。

为自适应地增强$F_{fused}$中的信息，我们对其进行进一步的处理。首先，将$F_{fused}$再次沿通道维度分割为$F_0$和$F_1$，并分别输入两个专用的注意力分支。每个分支内部是一个注意力模块，其结构如图\ref{att-blk}所示，具体而言，$F_0$和$F_1$同时输入两个并行的自适应平均池化层，分别产生尺寸为1和2的池化输出，池化结果经拼接后，通过一个由1x1卷积层、ReLU激活函数、第二个1x1卷积层和Sigmoid激活函数组成的序列，生成中间注意力权重，该过程可表述为：

\begin{equation}
   \text{Atten}(F_{i}) = \sigma(W_{2}\delta(W_{1} \cdot \text{Concat}[\text{Pool}_{1}(F_{i}), \text{Pool}_{2}(F_{i})])) 
\end{equation}

其中，$W_1$和$W_2$表示1x1卷积，$\delta$为ReLU函数，$\sigma$为Sigmoid函数。随后，两个分支产生的中间注意力权重被拼接，并通过SoftMax函数进行归一化，以计算最终的注意力权重：

\begin{equation}
    W_{atten} = \text{SoftMax}(\text{Concat}[\text{Atten}(F_{0}), \text{Atten}(F_{1})])
\end{equation}

最终，权重$W_{atten}$与特征图$F_{fused}$相乘，通过学习到的权重对不同通道的重要性进行调整。BAP模块通过并行的卷积处理与注意力机制，有效地融合了空间与上下文信息，使模型能够聚焦于特征图中信息最丰富的区域。与标准残差块相比，该设计能进行更有效的特征学习，最终提升了模型在复杂航拍场景下的小目标特征表征能力。

\xsubsection{双融合特征编码器}{Dual-Fusion Feature Encoder}
原始RT-DETR中的高效混合编码器通过仅使用单层Transformer编码器处理S5特征图，优化了基于注意力的特征融合，从而兼顾了计算效率与精度，而其基于CNN的跨尺度特征融合模块则遵循了传统的多尺度特征融合模式，通过卷积层整合特征，其中低分辨率特征仅通过最近邻上采样后便与高分辨率特征简单拼接。这种方法存在两个影响预测精度的问题：类内不一致性与边界偏移，此外，简单的插值操作也常常导致特征被过度平滑。

双融合特征编码器的结构如图\ref{bapdetr-arch}所示。自底向上和自顶向下路径中的融合块分别记为$F_{ij}^{bu}$和$F_{ij}^{td}$，其中$i$和$j$代表来自第$i$和第$j$个特征图的输入。在自底向上路径中，我们引入了一个额外的融合块$F_{2, 3}^{bu}$，将低层特征S2与其前一层的特征进行融合。低层特征S2包含更丰富的空间信息和小目标特征，这些细节能够有效提升复杂场景中小目标的定位与识别精度。将S2集成到特征融合过程中，可以极大改善检测性能，尤其是在小目标密集聚集的场景中。为进一步提升模型的小目标检测能力，我们直接将$F_{2, 3}^{bu}$的输出作为解码器的输入之一。同时，$F_{2, 3}^{bu}$的结果也被送入自顶向下路径进行下采样，并与自底向上路径各层的输出进行融合。在自顶向下路径中，我们移除了$F_{4, 5}^{td}$模块。这是因为经过深层卷积处理后，小目标特征极易被背景及其他目标影响，导致网络难以捕获差异化信息。最终，编码器的输出由原来的$F_{3, 4}^{bu}$, $F_{3, 4}^{td}$, $F_{4, 5}^{td}$变为$F_{2, 3}^{bu}$, $F_{2, 3}^{td}$, $F_{3, 4}^{td}$。此项调整在增加模型计算负载的同时也提升了精度，但实验表明这一以计算量换取精度的提升是值得的：具体而言，该修改增加了9.5 GFLOPs的计算量，带来了$AP$2.6\%的提升。相比之下，若直接将主干网络替换为ResNet-50而保持网络其余部分不变，计算量将增加76.0 GFLOPs，但$AP$仅能提升1.7\%。

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{fafm-arch.png}
\caption{频率感知融合模块示意图}
\label{fafm-arch}
\end{figure}

此外，我们将自底向上路径中的融合块$F_{3, 4}^{bu}$替换为频率感知融合模块。此修改增强了网络不同层级特征的整合能力，从而全面提升模型性能。FAFM的结构如图\ref{fafm-arch}所示，其设计灵感来源于Freqfusion\cite{Chen2024FreqFus}，采用了一个自适应低通滤波器生成器（Adaptive Low-Pass Filter Generator，ALPFG）和一个自适应高通滤波器生成器（Adaptive High-Pass Filter Generator，AHPFG）构成。ALPFG允许特征的低频成分通过并抑制高频成分，而AHPFG则用于从特征图中提取高频成分，从而克服标准卷积层的局限性。FAFM的输入来自两个不同尺寸的特征图，输出为一个与高分辨率输入特征图尺寸相同的融合特征。在融合前，两个输入特征图均经过一个1x1卷积层进行通道压缩，这有助于减少特征图的通道数并促进更有效的特征集成。我们采用CARAFE\cite{Wang2019CARAFE}作为低通滤波器，它能有效适应输入特征图的空间特性，在滤除噪声的同时增强模型保持重要空间信息的能力，FAFM的引入显著增强了网络内多层级特征的融合质量。然而，增加更多模块并不必然带来性能提升。通过实验，我们发现将$F_{3, 4}^{bu}$替换为FAFM是最优选择，具体结果将在实验部分讨论。

\xsubsection{损失函数}{Loss Function}
传统的IoU损失在某些情况下无法为网络优化提供有效梯度，例如当预测框与真实框不重叠，或一个框完全包含另一个框时。这两种情况在小目标检测中尤为常见。虽然CIoU和DIoU可以部分缓解此问题，但它们本质上仍基于IoU度量，对小目标的位置偏差极为敏感。具体而言，对于小目标，微小的位置偏移就会导致IoU值显著下降，而对于正常尺寸的目标，相同的偏移所引起的IoU变化则相对平缓。为克服这一问题，我们采用RNWD-CIoU来计算边界框损失，该损失由倒数归一化Wasserstein距离\cite{Wang2021NGWD}与标准的CIoU共同构成。我们将边界框建模为二维高斯分布$N(\mu,\sigma)$：
\begin{equation}
    \mu = \begin{bmatrix} x \\ y \end{bmatrix}, \sigma = \begin{bmatrix} \frac{w^2}{4} & 0 \\ 0 & \frac{h^2}{4} \end{bmatrix}
\end{equation}

其中，$(x,y)$为边界框中心坐标，$w$和$h$分别为边界框的宽度和高度。两个边界框之间的相似性可以转化为对应高斯分布之间的距离。两个高斯分布之间的Wasserstein距离平方计算如下：
\begin{equation}
    W_{2}^{2}(\mathcal{N}_{a}, \mathcal{N}_{b}) = \left\| \left[x_a, y_a, \frac{w_a}{2}, \frac{h_a}{2}\right]^T, \left[x_b, y_b, \frac{w_b}{2}, \frac{h_b}{2}\right]^T \right\|_{2}^{2}
\end{equation}

随后，我们利用倒数形式的归一化将Wasserstein距离映射到0至1的区间，作为边界框的相似性度量。与指数归一化形式相比，该归一化方法在计算效率上更具优势，且衰减更慢，有助于缓解梯度消失问题。
\begin{equation}
    \text{RNWD}(\mathcal{N}_{a}, \mathcal{N}_{b}) = \frac{1}{1 + \sqrt{W_{2}^{2}(\mathcal{N}_{a}, \mathcal{N}_{b})}}
\end{equation}

相比于IoU，RNWD在检测小目标时具有多项优势：尺度不变性、对位置偏移的平滑性，以及能够有效度量非重叠或相互包含的边界框之间的相似性。最终的边界框损失定义为：
\begin{equation}
    \text{Loss}_{\text{bbox}} = \lambda * \text{Loss}_{\text{ciou}} + (1 - \lambda) * \text{Loss}_{\text{RNWD}}
\end{equation}

其中，$\lambda$为权重系数，用于控制两部分损失的贡献比例。在处理密集小目标的场景时，适当降低$\lambda$值可以增强$Loss_{RNWD}$对整体损失的贡献，从而提升模型对微小目标的定位鲁棒性。


\xsection{实验结果与分析}{Experimental Results and Analysis}
\xsubsection{数据集与评价指标}{Datasets and Evaluation Metrics}
我们在两个无人机航拍图像基准数据集VisDrone\cite{Cao2021VisDroneDET2021}与UAVDT\cite{Du2018UAVDT}，以及一个面向微小目标检测的遥感数据集AI-TOD\cite{Wang2021AITOD}上进行了充分的实验。VisDrone-2019-DET的训练集包含6471张图像，验证集包含548张图像，所有图像均在不同高度和地点由无人机拍摄。每张图像使用边界框标注了十个预定类别：行人、人、汽车、厢式货车、公交车、卡车、摩托车、自行车、带篷三轮车以及三轮车。UAVDT训练集包含23258张图像，测试集包含15069张图像，该数据集涵盖了不同天气条件、飞行高度、拍摄角度和遮挡场景下的图像，包含汽车、公交车和卡车三个类别。AI-TOD数据集由28036张尺寸为800×800的图像构成，划分为训练集（11214张）、验证集（2804张）和测试集（14018张）。它包含八个目标类别：飞机、桥梁、储罐、船舶、游泳池、车辆、人和风车。

VisDrone数据集涵盖了复杂城市场景下各种无人机拍摄的情况，而UAVDT数据集则主要关注交通和人群场景。与VisDrone和UAVDT中无人机捕获的航拍图像不同，AI-TOD主要由光学遥感图像组成，这带来了拍摄角度、背景环境和目标尺度的差异，其目标平均尺寸仅为12.8像素，且86\%的目标小于16像素，这些特性使得AI-TOD比典型的无人机数据集更具挑战性。这些数据集对于无人机图像分析与目标检测领域的研究与开发具有重要价值，尤其是在涉及小目标和复杂背景的场景中。

我们采用广泛使用的COCO风格目标检测评估指标$AP$和$AP_{50}$来衡量精度，$AP$是通过对10个IoU阈值（从0.5到0.95，步长为0.05）上的精度取平均值计算得出。$AP_{50}$则是在IoU阈值为0.5时计算的平均精度。此外，为全面评估模型，我们还采用 GFLOPs、参数量 和 FPS 等指标来衡量模型的复杂度。GFLOPs是基于640×640的输入分辨率计算的。

\xsubsection{实现细节}{Implementation Details}
实验环境配置如下：CUDA 11.7，NVIDIA GeForce RTX 4090显卡，Python 3.9以及PyTorch 2.7.0。与传统的CNN模型相比，类DETR模型通常需要更长的训练时间，且收敛速度较慢。在单张RTX 4090 GPU上以批次大小为16进行训练时，可能会出现内存溢出错误，导致训练意外终止。若从断点恢复训练，最终模型的$AP$会出现明显下降。为确保训练过程的完整性并避免因内存溢出而中断，可采用的方案包括：使用多GPU并行训练，或在单GPU上减小批次大小。实验发现，使用双GPU训练所得的$AP$通常比单GPU训练低约0.2，这一下降可能归因于数据分布不均、梯度更新同步问题以及批量归一化效果减弱等因素。因此，我们最终选择了单GPU训练策略，设定批次大小为8，训练轮数为150。模型优化采用AdamW优化器，动量设置为0.9，权重衰减为0.0001。初始学习率和最终学习率分别为0.0001和1E-7。在训练过程中，采用了包括HSV色彩调整、平移及尺度变换在内的数据增强技术。

\begin{table*}[t]
  \centering
  \caption{Visdrone数据集结果对比}
  \label{tab1}
  \begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill}}llcccc}
    \toprule
    模型 & 出处 & GFLOPs& Params & $AP$ & $AP_{50}$ \\
    \midrule
    \multicolumn{6}{l}{基于CNN的通用检测器} \\
    \midrule
    YOLOv8-M & -- & 78.9 & 25.9 & 24.6 & 40.7 \\
    YOLOv9-M \cite{Wang2024Yolov9} & 2024 ECCV & 76.8 & 20.1 & 25.2 & 42.0 \\
    YOLOv10-M \cite{Wang2024Yolov10} & 2024 NeurIPS & 59.1 & 15.4 & 24.5 & 40.5 \\
    YOLOv11-M \cite{Jocher2023UltralyticsYOLO} & -- & 67.7 & 20.1 & 25.9 & 43.1 \\
    RetinaNet \cite{Lin2017FocalLoss} & 2017 ICCV & 88.5 & 35.6 & 21.8 & 39.3 \\
    FSAF \cite{Zhu2019FSAF} & 2019 CVPR & 246.7 & -- & 26.3 & 50.3 \\
    \midrule
    \multicolumn{6}{l}{基于Transformer的通用检测器} \\
    \midrule
    DETR \cite{Carion2020DETR} & 2020 ECCV & 187.0 & 60.0 & 24.1 & 40.1 \\
    Deformable DETR \cite{Zhu2020DeformableDETR} & 2020 ICLR & 173.0 & 40.0 & 27.1 & 42.2 \\
    Sparse DETR \cite{Roh2021SparseDETR} & 2022 ICLR & 121.0 & 40.9 & 27.3 & 42.5 \\
    RT-DETR \cite{Zhao202RTDETR} & 2024 CVPR & 136.0 & 42.0 & 28.4 & 47.0 \\
    \midrule
    \multicolumn{6}{l}{UAV图像专用检测器} \\
    \midrule
    HRDNet \cite{Liu2021HRDNet} & 2021 ICME & -- & 62.4 & 28.1 & 49.2 \\
    QueryDet \cite{Yang2022QueryDet} & 2022 CVPR & 212.0 & 36.2 & 28.3 & 48.1 \\
    CEASC \cite{Du2023CEASC} & 2023 CVPR & 150.1 & -- & 28.7 & 50.7 \\
    ClusDet \cite{Yang2019ClusteredOD} & 2019 ICCV & 207.0 & 30.2 & 26.7 & 50.6 \\
    NWD-RKA \cite{Xu2022NWD} & 2022 ISPRS & 246.0 & -- & 27.4 & 46.2 \\
    UAV-DETR \cite{Zhang2025UAVDETR} & 2024 arXiv & 170.0 & 42.0 & 31.5 & 51.1 \\
    DMNet \cite{Li2020DMNet} & 2020 CVPRW & 224.4 & -- & 28.2 & 47.6 \\
    \midrule
    \textbf{BAP-DETR-S} & -- & 140.2 & 25.0 & 33.2 & 51.6 \\
    \textbf{BAP-DETR-M} & -- & 179.7 & 34.8 & \textbf{35.3} & \textbf{53.5} \\
    \bottomrule
  \end{tabular*}
\end{table*}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{bap-scale-res.png}
\caption{各尺度精度对比}
\label{scale-res}
\end{figure}

\xsubsection{与SOTA的对比实验}{Comparison With State-of-The-Art Methods}
在本节中，我们在三个公开数据集（VisDrone、UAVDT 和 AI-TOD）上，将BAP-DETR与当前最先进的检测器进行对比分析，这些对比方法包括基于CNN和基于Transformer的通用检测器，以及专门为无人机图像设计的检测器。为展示模型的可扩展性以及在效率与精度间的平衡，我们提出了两个变体：BAP-DETR-S（使用带有BAPB的ResNet-18主干网络）和BAP-DETR-M（使用带有BAPB的ResNet-34主干网络）。

如表\ref{tab1}所示，BAP-DETR-S在所有对比的无人机图像检测器中实现了最低的计算复杂度，同时相比其基线模型，精度有4.8\% 的提升。如图\ref{scale-res}所示，与其他最先进的检测器相比，BAP-DETR-S在小目标和中等目标检测上展现了绝对优势。重要的是，我们计算量为179.7 GFLOPs的BAP-DETR-M，其计算负载与CEASC、QueryDet等近期先进方法相当甚至略低，但却提供了明显更优的检测性能。此外，与采用计算密集的聚焦检测（如ClusDet）的检测器相比，BAP-DETR-M在显著降低计算需求的同时，实现了更高的AP（8.6\% AP的提升）。尽管我们的双融合编码器引入了高分辨率特征图，但由此带来的计算负载增加，相对于复杂度相似或更高的同类方法，换取了可观的精度增益。

\begin{table*}[t]
  \centering
  \caption{UAVDT数据集结果对比}
  \label{tab2}
  \begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill}}llccccc}
    \toprule
    模型 & 出处 & GFLOPs & Params & $AP$ & $AP_{50}$ & FPS\\
    \midrule
    RT-DETR \cite{Zhao202RTDETR} & 2024 CVPR & 136.0 & 42.0 & 16.3 & 29.1 & 85.4\\
    ClusDet \cite{Yang2019ClusteredOD} & 2019 ICCV & 207.0 & 30.2 & 13.7 & 26.5 & 16.5\\
    DMNet \cite{Li2020DMNet} & 2020 CVPRW & 224.4 & -- & 14.7 & 24.6  & 15.4\\
    CEASC \cite{Du2023CEASC} & 2023 CVPR & 150.1 & -- & 17.1 & 30.9  & 64.6\\
    \midrule
    \textbf{BAP-DETR-S} & -- & 140.2 & 25.0 & 19.9 & 31.4 & 69.4 \\
    \textbf{BAP-DETR-M} & -- & 179.7 & 34.8 & \textbf{22.1} & \textbf{33.1} & 45.9\\
    \bottomrule
  \end{tabular*}
\end{table*}

\begin{table*}[t]
  \centering
  \caption{AI-TOD数据集结果对比}
  \label{tab3}
  \begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill}}llccccc}
    \toprule
    模型 & 出处 & GFLOPs & Params & $AP$ & $AP_{50}$ & FPS\\
    \midrule
    RT-DETR \cite{Zhao202RTDETR} & 2024 CVPR & 136.0 & 42.0 & 18.8 & 47.4 & 85.4\\
    HANet \cite{Guo2024HAN} & 2023 TCSVT & -- & 26.4 & 22.1 & 53.7 & --\\
    QueryDet  \cite{Yang2022QueryDet} & 2022 CVPR & 212.0 & 36.2 & 12.2 & 29.3  & 23.6\\
    NWD-RKA \citep{Xu2022NWD} & 2022 ISPRS & 246.0 & -- & 23.4 & 53.5  & 20.1\\
    DETR  \citep{Carion2020DETR} & 2020 ECCV & 187.0 & 60.0 & 18.4 & 41.4  & 27.8\\
    \midrule
    \textbf{BAP-DETR-S} & -- & 140.2 & 25.0 & 26.7 & 52.7 & 69.4 \\
    \textbf{BAP-DETR-M} & -- & 179.7 & 34.8 & \textbf{27.5} & \textbf{55.8} & 45.9\\
    \bottomrule
  \end{tabular*}
\end{table*}

为进一步证明BAP-DETR的泛化能力，我们还在用于微小目标检测的遥感数据集AI-TOD上进行测试，结果如表\ref{tab3}所示。BAP-DETR-S在AI-TOD上的$AP_{50}$达到52.7\%，优于基线RT-DETR。而BAP-DETR-M则以55.8\%的$AP_{50}$在所有对比方法中取得最高值。值得注意的是，尽管NWD-RKA在AI-TOD上表现出色，但其在VisDrone上的$AP_{50}$仅为46.2\%，远低于BAP-DETR-S。这些结果表明了我们的方法在不同航空成像场景下的泛化能力。

表\ref{tab2}和表\ref{tab3}还对比了BAP-DETR与其他方法的推理速度。BAP-DETR-S达到了69.4 FPS，比基线RT-DETR略慢，BAP-DETR-M以一定的速度换取了更高的精度，但仍保持45.9 FPS的实时性能，两个模型在速度上均优于其他专用检测器。总体而言，BAP-DETR在精度和速度之间取得了良好的平衡，适用于实时应用场景。

\xsubsection{消融实验}{Ablation Study}
双重注意力处理块（BAPB）、双融合编码器（Dual-fusion encoder）以及RNWD-CIoU损失函数是BAP-DETR的三个核心组件。为验证各模块的有效性，我们在VisDrone数据集上进行了消融实验。由于BAP-DETR-S采用ResNet-18作为主干网络，为公平对比，我们将基线模型RT-DETR的主干网络也替换为ResNet-18。

\subsubsection{核心组件消融实验}
表\ref{tab4}详细展示了各个组件对最终性能的贡献。双重注意力处理块和双融合编码器均能显著提升模型的$AP$值，同时也带来了计算负载的增加。这两者共同贡献了6.3\%的$AP$提升。相比之下，RNWD-CIoU损失函数在不增加模型参数量和计算复杂度的情况下，也为性能带来了有限的改进。与基线模型相比，BAP-DETR-S模型实现了6.5\%的$AP$提升，这证明了每个组件都对整体性能产生了积极贡献。

BAP-DETR-S确实比ResNet-18基线模型增加了计算负载，但从量化收益看，这些增加的计算量能带来可观的精度收益：BAP-DETR-S以增加80.2 GFLOPs的代价，换来了6.5\%的$AP$增益。若采用其他性能提升策略作为对比，例如将主干网络直接从ResNet-18替换为ResNet-50而保持其他部分不变，计算负载将增加76.0 GFLOPs，但$AP$仅能提升1.7\%。这充分证明，我们的结构修改在单位计算成本上带来了更优越的性能回报。

\begin{table}[h]
  \centering
  \caption{核心组件消融实验结果}
  \label{tab4}
  \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}ccccccc}
    \toprule
    \textbf{BAPB} & \textbf{Dual-fusion encoder} & \textbf{RNWD} & \textbf{$AP_{50}$} & \textbf{$AP$} & \textbf{GFLOPs} & \textbf{Params} \\
    \midrule
     & & & 44.6 & 26.7 & 60.0 & 20.0 \\
    \checkmark & & & 49.1 & 30.4 & 130.7 & 21.1 \\
    \checkmark & \checkmark & & 51.1 & 33.0 & 140.2 & 25.0 \\
    \checkmark & \checkmark & \checkmark & \textbf{51.6} & \textbf{33.2} & 140.2 & 25.0 \\
    \bottomrule
  \end{tabular*}
\end{table}

\subsubsection{频率感知融合模块消融实验}
为验证频率感知融合模块的有效性并确定其在编码器中的最佳配置，我们进行了一系列消融实验。原始RT-DETR编码器包含四个融合块（自底向上与自顶向下路径各两个）。在引入低层特征S2后，自底向上路径新增了一个融合块$ F_{2,3}^{bu}$，使得可替换的融合块变为三个：$ F_{2,3}^{bu}$, $F_{3,4}^{bu}$和$F_{4,5}^{bu}$。我们设计了四种实验配置：分别用FAFM单独替换上述三个融合块，以及同时替换所有三个融合块。实验结果如表\ref{tab5}所示。

\begin{table}[H]
  \centering
  \caption{频率感知融合模块消融实验结果}
  \label{tab5}
  \begin{tabular*}{0.75\textwidth}{@{\extracolsep{\fill}}ccccccc}
    \toprule
    \textbf{$\mathcal{F}^{bu}_{2,3}$} & \textbf{$\mathcal{F}^{bu}_{3,4}$} & \textbf{$\mathcal{F}^{bu}_{4,5}$} & \textbf{$AP_{50}$} & \textbf{$AP$} & \textbf{GFLOPs} & \textbf{Params} \\
    \midrule
     & & & 44.6 & 26.7 & 60.0 & 20.0 \\
    \checkmark & & & 46.8 & 29.2 & 60.9 & 20.3 \\
     & \checkmark & & \textbf{47.6} & \textbf{30.4} & 61.0 & 20.2 \\
     & & \checkmark & 46.6 & 29.1 & 60.8 & 20.2 \\
    \checkmark & \checkmark & \checkmark & 47.2 & 30.1 & 65.8 & 23.7 \\
    \bottomrule
  \end{tabular*}
\end{table}

实验结果表明，在各种配置中引入FAFM均能提升$AP$与$AP_{50}$，其中，在中层特征（S3-S4）对应的$F_{3,4}^{bu}$处应用FAFM效果最为显著，取得了30.4\%的$AP$和47.6\%的$AP_{50}$。单独替换三个融合块所产生的计算负载相对一致。然而，同时替换所有三个融合块虽进一步提高了计算复杂度，但性能增益并未成比例增加。这说明，虽然FAFM能增强特征融合，但累积添加多个模块可能导致计算资源的无谓消耗，而无法带来相应的精度提升。

这一反直觉结果的原因可能在于，添加多个FAFM模块可能会破坏网络固有的特征层次。FAFM的设计原理是：在高层特征图中衰减目标内部的高频分量以减少类内不一致性，同时在低层特征图中增强高频分量以维持清晰的边界。在S2这类低层特征中应用FAFM，不仅会放大有用的高频细节，也会放大噪声和背景干扰。当与其他FAFM模块的放大效应叠加时，会导致噪声累积，从而对模型产生干扰。而在高层特征中，连续应用FAFM会导致高频成分被过度衰减，产生过度平滑的特征，从而丢失精确定位所需的结构细节。中层特征S3和S4恰好包含了空间细节与语义信息的平衡，因此对频率感知增强的响应最为积极和有效。本实验证实了我们的设计选择：仅在关键的$F_{3,4}^{bu}$位置引入单一FAFM模块，是实现性能与效率最优平衡的最佳策略。

\subsubsection{RNWD消融实验}
将RNWD整合到现有的CIoU损失函数中，是网络的另一项改进。消融实验结果表\ref{tab6}展示了RNWD权重$\lambda$ 对模型性能的影响。结果表明，引入RNWD能带来性能指标的轻微提升：当$\lambda$值为0.3和0.7时，$AP_{50}$达到44.9\%，当$\lambda=0.7$时，$AP$达到26.8\%。与BAPB和FAFM带来的改进相比，RNWD的提升相对有限，但其优势在于不引入任何额外的模型参数或计算负载。

\begin{table}[h!]
  \centering
  \caption{RNWD权重系数消融实验结果}
  \label{tab6}
  \begin{tabular*}{0.75\textwidth}{@{\extracolsep{\fill}}lcc}
    \toprule
    \textbf{Loss function} & \textbf{$AP_{50}$} & \textbf{$AP$} \\
    \midrule
    CIOU & 44.6 & 26.7 \\
    CIOU+RNWD($\lambda$=0.3) & 44.9 & 26.6 \\
    CIOU+RNWD($\lambda$=0.5) & 44.8 & 26.7 \\
    CIOU+RNWD($\lambda$=0.7) & \textbf{44.9} & \textbf{26.8} \\
    CIOU+RNWD($\lambda$=0.9) & 44.8 & 26.7 \\
    \bottomrule
  \end{tabular*}
\end{table}


\begin{table}[h!]
  \centering
  \caption{RNWD消融实验结果}
  \label{tab7}
  \begin{tabular*}{0.75\textwidth}{@{\extracolsep{\fill}}lcc}
    \toprule
    \textbf{Loss function} & \textbf{$AP_{50}$} & \textbf{$AP$} \\
    \midrule
    CIOU & 44.6 & 26.7 \\
    +GWD & 43.8 & 26.2 \\
    +GWD exp norm  & 44.9 & 26.6 \\
    +KL Divergence  & 44.7 & 26.8 \\
    +RNWD & \textbf{44.9} & \textbf{26.8} \\
    \bottomrule
  \end{tabular*}
\end{table}

为更进一步验证RNWD的有效性，我们将其与IoU系列之外的其他先进损失函数进行了比较，包括GWD、指数归一化GWD以及KL散度损失。如表\ref{tab7}所示，GWD因其快速增长的梯度趋势而对大误差过于敏感，导致其性能不佳。指数归一化GWD在$AP_{50}$指标上表现出竞争力，但在$AP$上略低于RNWD，与此同时，KL散度损失取得了与RNWD相近的$AP$，却在$AP_{50}$上表现不足。RNWD在两个指标上均提供了最均衡、稳定的性能，验证了其设计的有效性。


\xsubsection{可视化结果}{Visualization}
为直观展示BAP-DETR的有效性，图\ref{fig7}中呈现了若干具有代表性的检测样例。图中第一、二行分别展示了基线模型与BAP-DETR模型的检测结果，第三、四行则分别展示了两者对应的、基于边界框预测进行反向传播所生成的热力图。从图中可以直观观察到，基线模型中存在的多个误检与漏检，均被BAP-DETR有效消除。与基线模型相比，模型在热力图上表现出对密集小目标及其周围环境更强的聚焦能力。这表明，BAP-DETR不仅提升了分类性能，还增强了对上下文线索的敏感度，使得模型对关键区域的关注更加集中和精准，有效抑制了背景噪声的干扰。通过可视化对比，BAP-DETR在复杂航拍场景下对小目标，尤其是密集分布的小目标，展现出了更精确的感知能力。

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{bapdetr-vis.png}
    \caption{第一行呈现了基线方法的检测结果，第二行则展示了BAP-DETR的检测结果。其中，绿色、蓝色和红色边界框分别表示正确检测、误检与漏检。第三行和第四行分别为基线方法与BAP-DETR的热力图可视化结果。}
    \label{fig7}
\end{figure*}

\xsection{本章小结}{Chapter Summary}
本章针对机载光电系统目标检测中的核心挑战，提出了一种面向可见光航拍图像的高性能小目标检测网络BAP-DETR。通过引入双重注意力处理块，BAP-DETR能够同时捕获对小目标检测至关重要的细粒度特征与全局上下文信息。结合频率感知融合模块的双融合编码器，在有效整合多尺度特征的同时，保留了高分辨率的空间信息。RNWD-CIoU损失函数解决了传统IoU度量在密集小目标场景下的局限。BAP-DETR通过两个变体（侧重精度的BAP-DETR-M与侧重效率的BAP-DETR-S）实现了灵活的速度精度调节。两个变体均在精度上超越了现有方法，其中侧重效率的变体相比专用的无人机图像检测器，计算负载显著降低。在三个公开航拍数据集上的广泛评估证实，BAP-DETR实现了业界领先的效率精度平衡。